# -*- coding: utf-8 -*-
"""ann_pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mov2wTSpdl2BnQ8FsCLy5eEKl0NHNwc4

**Human Stress Level Classification using Artificial Neural Network (ANN)**
"""

#Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

#Load The Dataset
from google.colab import files
uploaded=files.upload()

df = pd.read_csv("stress_level_balanced_ann_dataset.csv")
df.head()

#Statistical summry
df.describe()

df.info()

# Feature Target Split
X = df.drop("stress_level", axis=1)
y = df["stress_level"].astype(int)

#Correlation Heatmap
sns.heatmap(df.corr(), annot=True)
plt.show()

"""The correlation heatmap shows that work hours,screen time are positively correlated with stress level, while sleep hours and physical activity are negatively correlated."""

# Train Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

#Feature Scaling
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#ANN model
model = Sequential()
# Input + First Hidden Layer
model.add(Dense(8, activation='relu', input_shape=(4,)))
# Second Hidden Layer
model.add(Dense(4, activation='relu'))
# Output Layer
model.add(Dense(3, activation='softmax')) #for multiclass :- softmax

# Model Compilation
model.compile(
    optimizer='adam', #fast
    loss='sparse_categorical_crossentropy', #target is catrgorical label
    metrics=['accuracy'] #performance metrices
    )

#Early Stopping (Overfitting Control Technique)
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',        # metric to monitor
    patience=8,                # number of epochs to wait
    restore_best_weights=True  # revert to best model
)

#Model Training
history = model.fit(
    X_train,
    y_train,
    epochs=150,          # high max, but stops early
    batch_size=8,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

#Training Performance Visualization
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.show()

"""if both decrease:- good fit

training dec val inc:- overfit

training inc val inc:- underfit
"""

#Generalization Gap Analysis
train_acc = history.history['accuracy'][-1]
val_acc = history.history['val_accuracy'][-1]

print("Gap:", abs(train_acc - val_acc))

"""The generalization gap is nearly zero, which indicates the model is neither overfitting nor underfitting and performs consistently on unseen data.

0.0 ‚Äì 0.05	: Good fit (my case)

0.05 ‚Äì 0.15 : Slight overfitting

"""

# Predict probabilities
y_pred = model.predict(X_test)
#probabilities to class labels kyuki matrices required label
y_pred_classes = np.argmax(y_pred, axis=1)

# performce measure metrics:- accuracy score
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred_classes) #use actual labels
print("Test Accuracy:", round(acc * 100 ,2), "%")

"""accuracy score close to 100% indicates that , demonstrating strong predictive performance."""

# log loss
from sklearn.metrics import log_loss
logloss = log_loss(y_test, y_pred) #use probabilities
print("Log Loss:", logloss)

"""Log loss  0.006 indicates that the model makes highly confident and correct probability predictions, which reflects excellent performance."""

#Predicting on New Data

print("Enter the following information to predict stress level:")
sleep = int(input("Enter sleep hours per day (0‚Äì24): "))
work = int(input("Enter work hours per day (0‚Äì24): "))
screen = int(input("Enter screen time hours per day (0‚Äì24): "))
activity = int(input("Enter physical activity hours per day (0‚Äì5): "))
#combining into a Dataframe
user_data = pd.DataFrame(
    [[sleep, work, screen, activity]],
    columns=["sleep_hours", "work_hours", "screen_time", "physical_activity"]
)
#Scaling new data
user_data_scaled = scaler.transform(user_data)
#Predicting on new data
prediction = model.predict(user_data_scaled)
# Convert probabilities to class
predicted_class = np.argmax(prediction)
# Label Mapping
labels = {
    0: "Low Stress üòä",
    1: "Medium Stress üòê",
    2: "High Stress üòü"
}
# Result
print("\nüìä Prediction Result")
print("--------------------")
print("Predicted Stress Level:", labels[predicted_class])
print("Prediction Confidence:", round(np.max(prediction) * 100, 2), "%")

# High work hours ‚Üí increase stress
# High screen time ‚Üí increase stress
# High physical activity ‚Üí decrease stress
# High sleep hours ‚Üí decrease stress